\section{Exercise 11}
notes
a = learningrate
e = exploration/mutation rate
y = discount rate

newQ = (1 - a)oldQ + (a)sample
sample = reward + y(futureQs)
if random < e: random step
else best step

raw:steps to complete/100
dif:|raw[i] - raw[i - 1]|

all std, a = 1.0
raw:15,28,38,48,58,69,78,88
dif:15,13,10,10,10,11,9,10
all std, a = 0.8(std)
raw:21,32,44,56,67,77,87,98
dif:21,11,12,12,11,10,10,11
all std, a = 0.586
raw:19,31,42,52,64,74,85,96
dif:19,12,11,10,12,10,11,11
all std, a = 0.333
raw:20,34,48,62,76,89,100,112
dif:20,14,14,14,14,13,11,12
all std, a = 0.111
raw:55,76,95,111,124,137,150,165
dif:55,21,19,16,13,13,13,15

all std, e = 0.8
raw:38,66,88,123,154,185,214,244
dif:38,28,22,35,31,31,29,30
all std, e = 0.5
raw:18,30,41,53,65,75,85,96
dif:18,12,11,12,12,10,10,11
all std, e = 0.333
raw:18,29,38,48,56,65,74,82
dif:18,11,9,10,8,9,9,8
all std, e = 0.2
raw:30,39,46,54,62,69,76,83
dif:30,9,7,8,8,7,7,7
all std, e = 0.081
raw:44,56,66,76,85,95,106,115
dif:44,12,10,10,9,10,11,9

high a, low e = learns fast from actions performered, but tries little new things.
Since he does not know anything to start with, he needs to randomly come across a good action at the right time.
The low e value holds the crawler back from learning as fast as he could.
high a, high e = learns fast from actions performered, and also tries many things(random) out.
This combination lets the crawler learn rapidly at the beginning. However, when he has learned to walk, the high e
holds him back from performing at his best. Because a high e brings a high probability for a random action, his
behaviour is quite random. Now he knows the right actions, this is not needed anymore. Many steps are now wasted, the random
actions do not contribute walking faster and could even push him back at bit.
low a, low e = This is a very slow learning combination. There is a low chance that he will randomly do something.
And if he randomly does something right, it picks it up slow because of the low a. But when it, eventually, learned how to walk
he performs great. He will not disrupt his walk with randomness, and random bad actions do not influence him because of
the low a.
low a, high e = This one is weird. He tries many things, but learns from them slowly. When he learned to crawl, randomness
still disrupts his speed. But this time it will not affect the learned policy so much.

The best thing is to have a high e and a high a, so he learns fast. He will start walking in no time. Then you can decrease
the e and a slowly as he gets better. At the end set them to zero as the learning has finalised. Then there will be no
randomness to disrupt the crawl, and the crawler will be at his fastest.