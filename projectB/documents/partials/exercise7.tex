\section{Exercise 7}
\subsection{}
Both files contain a class which can be extended to implement an agent. Both of these classes implement a different base class / interface. \\
The one in file \textit{valueIterationAgents.py} is meant to be implemented for agents which estimate the Q-values and values for an environment using a Markov Decision Process, which is done before acting.
The other in file \textit{qlearningAgents.py}  is meant to be implemented for agents that estimate Q-values from policies rather than from a model.

\subsection{}
\begin{itemize}
  \item In reinforcement learning transition probabilities are unknown. Which is one of the reasons why MDP can't be used in such cases.
  \item In reinforcement learning transition rewards are unknown. These are learned with experience and can differ every iteration.
\end{itemize}

\subsection{}
GridWorld implementation differs in how terminal state and rewards is handled. In contrary to slides the rewards are set per grid cell basis. Which means a cell can give a reward and not a be a terminal state (although only action possible in a cell with a reward is a terminal state). \\
Terminal states do not belong to any cell, nor do they give any rewards. They are implemented as actions. These are only to indicate when the iteration should break. In the slides they behave as a cell, but a final one.

\subsection{}
\textit{\say{A terminal state is a state that once reached causes all further action of the agent to cease.}}\\
\textit{Source: \url{http://burlap.cs.brown.edu/tutorials/bd/p1.html}}. \smallskip

Both slides and implementation use this definition. Since no actions can be taken once a terminal state is reached.
