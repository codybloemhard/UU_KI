\section{Exercise 8}
\subsection{ex8-a}
The perceptron classifier for pacman has a score of 88/100 in on the validation set. Is has a score of 84/100 on the test set. This means it might be a little overfitted, but it can generalize fairly well.
\subsection{ex8-b}
StopAgent: The stopagent just wants to stand still. This is very easy for the perceptron, it will increase the weights for stopping, and lower the weights of all the other things. The perceptron does not need any aditional features, for it can achieve a 100\% score without any features. \\
FoodAgent: The foodagent just wants to eat food. The distance to the closest food of the succesor state looks like a good feature. A high value on this feature means that the agent is far away from food, and a low value means it is close to food. The agents can now try to optimise for a low value, meaning it will go towards the food. \\
SuicideAgent: The suicideagent just wants to crash into a ghost to die. The distance to the closest ghost can be a helpful feature. It can again optimise for a low value and learn to move into the ghost. There is a better chance of getting killed if the agent is in a area with multiple ghosts. By adding the feature of the sum of the distances to the ghosts could help the agent to have a bias towards more crowded places. \\
ContestAgent: The contestagent wants to play the game to the best of it's ability. The distance to the closest enemy is again important, but this time to avoid the ghosts. The distance to the closest food is also important again, because it really needs to eat the food to win. The distance to the closest power pallet could be a good feature here. We want to stimulate pacman to eat those, as it makes him invincible for a short time. That can help with winning the game. 
\subsection{ex8-c}
StopAgent: It does not matter what feature you give it or, if you give it a feature at all. I gave it the score of the game as feature. \\
FoodAgent: The distance to the nearest food did not work out well. Normalizing the distance worked out. Our function for doing so is norm(x) = 1.0 / (x + 1). This is very simple and guards against division through zero where x > 0. I rare ocations, when a piece of food is alone in an area, eating it willincrease the distance to the next food significantly. An attempt to fix this was by setting the feature's value to 1 if the succesor state has less food than the current state. By doing so the feature always has a higher or equal value for an action where pacman eats food verus an action where pacman does not eat. However, this change had a small negative effect on the performance, and it was removed. \\
SuicideAgent: Here we took the distance to the nearest ghost from pacman. The raw distance did not work again, so it is normalized again. This was enough to get a 90\% test score. The other feature, the sum of the distances to all the ghosts was also normalized. It did help a little bit, the test score went up to 93\%. Interestingly, the validation score was lower than the test score with 87\% at first and only gained 0.6\%. \\
ContestAgent: The feature of the normalized distance to the nearest food had a very positive effect on the score of the agent, as expected. If the agent does not eat, he can not win. The normalized distance to the nearest ghost had a negative influence on the score. This is because it might get in the way of some other features that were added. The same goes with the normalized distance to the closest power pallet, it decreased the score. A feature that did have a big positive effect was the normalized absolute difference of the succesor's state score and the current state's score. This can be because it indicates many things, although indirectly. For example, eating food yields a higher value than not eating food. Getting hit by a ghost yields lower value than not getting hit. Eating a power pallet yields a higher score than not eating one. All the good things here lead to a higher value. More importantly than if it leads to a high or low value, all the good things lead the value into the same direction. This feature is a strong indicator if the succesor state is a good one. The perceptron had trouble with all this information in seperate features, but combining seems the way to go. There is another small feature, it's value is 1 if the succesor state is winning, 0 if it is losing, and else it is 0.5. This has a very small but positive effect on the score. The idea is to advice the agent against really stupid moves like walking into a ghost or walking past the last piece of food.
